{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country_id</th>\n",
       "      <th>year</th>\n",
       "      <th>item_id</th>\n",
       "      <th>preference</th>\n",
       "      <th>GDP_growth</th>\n",
       "      <th>pop_growth</th>\n",
       "      <th>diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2013</td>\n",
       "      <td>16.0</td>\n",
       "      <td>6.321578e-07</td>\n",
       "      <td>0.101959</td>\n",
       "      <td>0.287706</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2013</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1.799218e-06</td>\n",
       "      <td>0.101959</td>\n",
       "      <td>0.287706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2013</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.905108e-06</td>\n",
       "      <td>0.101959</td>\n",
       "      <td>0.287706</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2013</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.577259e-06</td>\n",
       "      <td>0.101959</td>\n",
       "      <td>0.287706</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>2013</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.458826e-06</td>\n",
       "      <td>0.101959</td>\n",
       "      <td>0.287706</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376526</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.467287e-07</td>\n",
       "      <td>0.132152</td>\n",
       "      <td>0.233088</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376527</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021</td>\n",
       "      <td>85.0</td>\n",
       "      <td>3.288542e-05</td>\n",
       "      <td>0.132152</td>\n",
       "      <td>0.233088</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376528</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021</td>\n",
       "      <td>87.0</td>\n",
       "      <td>4.543334e-05</td>\n",
       "      <td>0.132152</td>\n",
       "      <td>0.233088</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376529</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.134952e-05</td>\n",
       "      <td>0.132152</td>\n",
       "      <td>0.233088</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376530</th>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>2021</td>\n",
       "      <td>96.0</td>\n",
       "      <td>6.696923e-07</td>\n",
       "      <td>0.132152</td>\n",
       "      <td>0.233088</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>376531 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         country_id  year  item_id    preference  GDP_growth  pop_growth  diff\n",
       "0       Afghanistan  2013     16.0  6.321578e-07    0.101959    0.287706     0\n",
       "1       Afghanistan  2013     19.0  1.799218e-06    0.101959    0.287706     1\n",
       "2       Afghanistan  2013     21.0  6.905108e-06    0.101959    0.287706     0\n",
       "3       Afghanistan  2013     27.0  2.577259e-06    0.101959    0.287706     0\n",
       "4       Afghanistan  2013     28.0  1.458826e-06    0.101959    0.287706     1\n",
       "...             ...   ...      ...           ...         ...         ...   ...\n",
       "376526     Zimbabwe  2021     82.0  2.467287e-07    0.132152    0.233088     1\n",
       "376527     Zimbabwe  2021     85.0  3.288542e-05    0.132152    0.233088     1\n",
       "376528     Zimbabwe  2021     87.0  4.543334e-05    0.132152    0.233088     1\n",
       "376529     Zimbabwe  2021     90.0  1.134952e-05    0.132152    0.233088     0\n",
       "376530     Zimbabwe  2021     96.0  6.696923e-07    0.132152    0.233088     1\n",
       "\n",
       "[376531 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = pd.read_csv('../processed/FM_classifciation 3.csv')\n",
    "ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year encoding\n",
    "year_dict = {}\n",
    "for i in set(ratings['year']):\n",
    "    year_dict[i] = len(year_dict)\n",
    "n_year = len(year_dict)                  \n",
    "\n",
    "# Country encoding\n",
    "country_dict = {}\n",
    "start_point = n_year                     \n",
    "for i in set(ratings['country_id']):\n",
    "    country_dict[i] = len(country_dict)\n",
    "n_country = len(country_dict)            \n",
    "start_point += n_country                 \n",
    "\n",
    "# Item encoding\n",
    "item_dict = {}\n",
    "start_point = n_country                  \n",
    "for i in set(ratings['item_id']):\n",
    "    item_dict[i] = start_point + len(item_dict)\n",
    "n_item = len(item_dict)                  \n",
    "start_point += n_item                    \n",
    "\n",
    "# preference encoding\n",
    "pre_index = start_point\n",
    "start_point += 1\n",
    "\n",
    "# GDP_growth encoding   \n",
    "gdp_index = start_point\n",
    "start_point += 1\n",
    "\n",
    "# pop_growth encoding\n",
    "pop_index = start_point\n",
    "start_point += 1\n",
    "num_x = start_point                     # 전체 변수의 숫자 (년도 + 국가 개수 + 품목 개수 + GDP_growth + pop_growth, preference) 총 311개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding  0  cases...\n",
      "Encoding  30000  cases...\n",
      "Encoding  60000  cases...\n",
      "Encoding  90000  cases...\n",
      "Encoding  120000  cases...\n",
      "Encoding  150000  cases...\n",
      "Encoding  180000  cases...\n",
      "Encoding  210000  cases...\n",
      "Encoding  240000  cases...\n",
      "Encoding  270000  cases...\n",
      "Encoding  300000  cases...\n",
      "Encoding  330000  cases...\n",
      "Encoding  360000  cases...\n"
     ]
    }
   ],
   "source": [
    "# Generate X data\n",
    "data = []\n",
    "y = []\n",
    "for i in range(len(x)):\n",
    "    case = x.iloc[i]\n",
    "    x_index = []\n",
    "    x_value = []\n",
    "    x_index.append(year_dict[case['year']])        # year id one-hot encoding\n",
    "    x_value.append(1.)                             # 있으면 1\n",
    "    x_index.append(country_dict[case['country_id']])  # country id one-hot encoding\n",
    "    x_value.append(1.)\n",
    "    x_index.append(item_dict[case['item_id']])     # item id one-hot encoding\n",
    "    x_value.append(1.)\n",
    "    x_index.append(pre_index)                      # preference\n",
    "    x_value.append(case['preference'])\n",
    "    x_index.append(gdp_index)                      \n",
    "    x_value.append(case['GDP_growth'])\n",
    "    x_index.append(pop_index)\n",
    "    x_value.append(case['pop_growth'])\n",
    "    data.append([x_index, x_value])                # 만들어진 한줄의 데이터를([[year_index, country_index, item_index, gdp_index, pop_index], [1, 1, 1, 1, 1]] 형태) data에 저장\n",
    "    y.append(case['diff'])                         # 종속변수\n",
    "    if (i % 30000) == 0:\n",
    "        print('Encoding ', i, ' cases...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145693 38975\n"
     ]
    }
   ],
   "source": [
    "### Train-Test split\n",
    "trainset = data[:145693]\n",
    "testset = data[145693:]   # 2017년 데이터\n",
    "print(len(trainset), len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145693\n"
     ]
    }
   ],
   "source": [
    "# y데이터에서 2017년 데이터 지우기\n",
    "y = y[:145693]\n",
    "print(len(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactorizationMachine():\n",
    "    def __init__(self, N, K, data, y, test_data, lr, l2_reg, l2_lambda, epoch, early_stop_window, train_ratio=0.75):\n",
    "        \"\"\"\n",
    "        :param k: number of latent vector\n",
    "        :param lr: learning rate\n",
    "        :param l2_reg: bool parameter for L2 regularization\n",
    "        :param l2_lambda: lambda of L2 regularization\n",
    "        :param epoch: training epoch\n",
    "        \"\"\"\n",
    "        self._K = K\n",
    "        self._N = N\n",
    "        self._data = data\n",
    "        self._y = y\n",
    "        self._test_data = test_data\n",
    "        self._lr = lr\n",
    "        self._l2_reg = l2_reg\n",
    "        self._l2_lambda = l2_lambda\n",
    "        self._epoch = epoch\n",
    "        self._early_stop_window = early_stop_window\n",
    "        self._valid_loss_list = []\n",
    "        # Train/Valid/Test 분리\n",
    "        cutoff = int(train_ratio * len(data))\n",
    "        self.train_x = data[:cutoff]\n",
    "        self.valid_x = data[cutoff:]\n",
    "        self.train_y = y[:cutoff]\n",
    "        self.valid_y = y[cutoff:]\n",
    "        self.test_x = test_data\n",
    "\n",
    "    def _load_dataset(self):\n",
    "        # init FM vectors\n",
    "        self._init_vectors()\n",
    "        print(\"Finish init FM vectors.\")\n",
    "\n",
    "    def _init_vectors(self):\n",
    "        # w 초기화\n",
    "        self.w = np.random.normal(scale=1./self._N, size=(self._N))          # 사이즈는 변수의 수 만큼\n",
    "        # v 초기화\n",
    "        self.v = np.random.normal(scale=1./self._K, size=(self._N, self._K))  # 사이즈는 (변수의 수 X K)\n",
    "        self.train_x_data = []\n",
    "        self.train_y_data = np.zeros((len(self.train_x)))\n",
    "        self.valid_x_data = []\n",
    "        self.valid_y_data = np.zeros((len(self.valid_x)))\n",
    "\n",
    "        self.train_y_data = np.array(self.train_y)\n",
    "        for n, row in enumerate(self.train_x):\n",
    "            self.train_x_data.append([np.array(row[0]), np.array(row[1])])\n",
    "        \n",
    "        self.valid_y_data = np.array(self.valid_y)\n",
    "        for n, row in enumerate(self.valid_x):\n",
    "            self.valid_x_data.append([np.array(row[0]), np.array(row[1])])\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Train FM model by Gradient Descent with L2 regularization\n",
    "        \"\"\"\n",
    "        self._load_dataset()\n",
    "        for epoch_num in range(1, self._epoch):\n",
    "            train_y_hat = self.predict(data=self.train_x_data)\n",
    "            valid_y_hat = self.predict(data=self.valid_x_data)\n",
    "            train_loss = self._get_loss(y_data=self.train_y_data, y_hat=train_y_hat)\n",
    "            valid_loss = self._get_loss(y_data=self.valid_y_data, y_hat=valid_y_hat)\n",
    "            train_auc = roc_auc_score(self.train_y_data, train_y_hat)\n",
    "            valid_auc = roc_auc_score(self.valid_y_data, valid_y_hat)\n",
    "            self._print_learning_info(epoch=epoch_num, train_loss=train_loss, valid_loss=valid_loss,\n",
    "                                      train_auc=train_auc, valid_auc=valid_auc)\n",
    "            if self._check_early_stop(valid_loss=valid_loss):\n",
    "                print(\"Early stop at epoch:\", epoch_num)\n",
    "                return 0\n",
    "\n",
    "            self._stochastic_gradient_descent(self.train_x_data, self.train_y_data)\n",
    "\n",
    "    def predict(self, data):\n",
    "        \"\"\"\n",
    "        Implementation of FM model's equation on O(kmd)\n",
    "\n",
    "        -----\n",
    "        Numpy array shape : (n, [index of md], [value of md])\n",
    "        md : none-zero feature\n",
    "        \"\"\"  \n",
    "        num_data = len(data)\n",
    "        scores = np.zeros(num_data)\n",
    "        for n in range(num_data):\n",
    "            feat_idx = data[n][0]\n",
    "            val = data[n][1]\n",
    "\n",
    "            # linear feature score\n",
    "            linear_feature_score = np.sum(self.w[feat_idx] * val)\n",
    "\n",
    "            # factorized feature score\n",
    "            vx = self.v[feat_idx] * (val.reshape(-1, 1))\n",
    "            cross_sum = np.sum(vx, axis=0)\n",
    "            square_sum = np.sum(vx * vx, axis=0)\n",
    "            cross_feature_score = 0.5 * np.sum(np.square(cross_sum) - square_sum)\n",
    "\n",
    "            # Model's equation\n",
    "            scores[n] = linear_feature_score + cross_feature_score\n",
    "\n",
    "        # Sigmoid transformation for binary classification\n",
    "        scores = 1.0 / (1.0 + np.exp(-scores))\n",
    "        return scores\n",
    "\n",
    "    def inference(self, data):\n",
    "        \n",
    "        self.test_x_data = []\n",
    "        for n, row in enumerate(self.test_x):\n",
    "            self.test_x_data.append([np.array(row[0]), np.array(row[1])])\n",
    "\n",
    "        num_data = len(data)\n",
    "        scores = np.zeros(num_data)\n",
    "        for n in range(num_data):\n",
    "            feat_idx = data[n][0]\n",
    "            val = np.array(data[n][1])\n",
    "\n",
    "            # linear feature score\n",
    "            linear_feature_score = np.sum(self.w[feat_idx] * val)\n",
    "\n",
    "            # factorized feature score\n",
    "            vx = self.v[feat_idx] * (val.reshape(-1, 1))\n",
    "            cross_sum = np.sum(vx, axis=0)\n",
    "            square_sum = np.sum(vx * vx, axis=0)\n",
    "            cross_feature_score = 0.5 * np.sum(np.square(cross_sum) - square_sum)\n",
    "\n",
    "            # Model's equation\n",
    "            scores[n] = linear_feature_score + cross_feature_score\n",
    "\n",
    "        # Sigmoid transformation for binary classification\n",
    "        scores = 1.0 / (1.0 + np.exp(-scores))\n",
    "        print(scores)\n",
    "        return scores\n",
    "\n",
    "    def _get_loss(self, y_data, y_hat):\n",
    "        \"\"\"\n",
    "        Calculate loss with L2 regularization (two type of coeficient - w,v)\n",
    "        \"\"\"\n",
    "        l2_norm = 0\n",
    "        if self._l2_reg:\n",
    "            w_norm = np.sqrt(np.sum(np.square(self.w)))\n",
    "            v_norm = np.sqrt(np.sum(np.square(self.v)))\n",
    "            l2_norm = self._l2_lambda * (w_norm + v_norm)\n",
    "        return -1 * np.sum( (y_data * np.log(y_hat)) + ((1 - y_data) * np.log(1 - y_hat)) ) + l2_norm\n",
    "\n",
    "    def _check_early_stop(self, valid_loss):\n",
    "        self._valid_loss_list.append(valid_loss)\n",
    "        if len(self._valid_loss_list) > 5:\n",
    "            prev_loss = self._valid_loss_list[len(self._valid_loss_list) - self._early_stop_window]\n",
    "            curr_loss = valid_loss\n",
    "            if prev_loss < curr_loss:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def _print_learning_info(self, epoch, train_loss, valid_loss, train_auc, valid_auc):\n",
    "        print(\"epoch:\", epoch, \"||\", \"train_loss:\", train_loss, \"||\", \"valid_loss:\", valid_loss,\n",
    "              \"||\", \"Train AUC:\", train_auc, \"||\", \"Test AUC:\", valid_auc)\n",
    "\n",
    "    def _stochastic_gradient_descent(self, x_data, y_data):\n",
    "        \"\"\"\n",
    "        Update each coefs (w, v) by Gradient Descent\n",
    "        \"\"\"\n",
    "        for data, y in zip(x_data, y_data):\n",
    "            feat_idx = data[0]\n",
    "            val = data[1]\n",
    "            vx = self.v[feat_idx] * (val.reshape(-1, 1))\n",
    "\n",
    "            # linear feature score\n",
    "            linear_feature_score = np.sum(self.w[feat_idx] * val)\n",
    "\n",
    "            # factorized feature score\n",
    "            vx = self.v[feat_idx] * (val.reshape(-1, 1))\n",
    "            cross_sum = np.sum(vx, axis=0)\n",
    "            square_sum = np.sum(vx * vx, axis=0)\n",
    "            cross_feature_score = 0.5 * np.sum(np.square(cross_sum) - square_sum)\n",
    "\n",
    "            # Model's equation\n",
    "            score = linear_feature_score + cross_feature_score\n",
    "            y_hat = 1.0 / (1.0 + np.exp(-score))\n",
    "            cost = y_hat - y\n",
    "\n",
    "            if self._l2_reg:\n",
    "                self.w[feat_idx] = self.w[feat_idx] - cost * self._lr * (val + self._l2_lambda * self.w[feat_idx])\n",
    "                self.v[feat_idx] = self.v[feat_idx] - cost * self._lr * ((sum(vx) * (val.reshape(-1, 1)) - (vx * (val.reshape(-1, 1)))) + self._l2_lambda * self.v[feat_idx])\n",
    "            else:\n",
    "                self.w[feat_idx] = self.w[feat_idx] - cost * self._lr * val\n",
    "                self.v[feat_idx] = self.v[feat_idx] - cost * self._lr * (sum(vx) * (val.reshape(-1, 1)) - (vx * (val.reshape(-1, 1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish init FM vectors.\n",
      "epoch: 1 || train_loss: 75935.69517890604 || valid_loss: 25658.310164676364 || Train AUC: 0.5133722167803325 || Test AUC: 0.505717204027838\n",
      "epoch: 2 || train_loss: 68612.78989631005 || valid_loss: 23219.077499006704 || Train AUC: 0.6142979765339635 || Test AUC: 0.5710439087939472\n",
      "epoch: 3 || train_loss: 67394.20125948155 || valid_loss: 22751.17992477818 || Train AUC: 0.6437986217536693 || Test AUC: 0.597903303132004\n",
      "epoch: 4 || train_loss: 66747.64656017786 || valid_loss: 22550.799821060016 || Train AUC: 0.656323374743282 || Test AUC: 0.6091990010103477\n",
      "epoch: 5 || train_loss: 66346.46619872343 || valid_loss: 22449.606174769204 || Train AUC: 0.6630126400428296 || Test AUC: 0.6151947649255572\n",
      "epoch: 6 || train_loss: 66072.65938320672 || valid_loss: 22389.70231661666 || Train AUC: 0.6673985140578118 || Test AUC: 0.6189080706102865\n",
      "epoch: 7 || train_loss: 65871.90404772111 || valid_loss: 22350.315160652714 || Train AUC: 0.6705460527237596 || Test AUC: 0.6213531812362076\n",
      "epoch: 8 || train_loss: 65716.58040505527 || valid_loss: 22322.87690614224 || Train AUC: 0.6729074715050596 || Test AUC: 0.6230804996944763\n",
      "epoch: 9 || train_loss: 65591.35237470463 || valid_loss: 22303.358218335394 || Train AUC: 0.6747676882586289 || Test AUC: 0.6243305481675852\n",
      "epoch: 10 || train_loss: 65486.945671264664 || valid_loss: 22289.573334274388 || Train AUC: 0.6762907090928499 || Test AUC: 0.6252253821240745\n",
      "epoch: 11 || train_loss: 65397.28080736918 || valid_loss: 22280.155766356962 || Train AUC: 0.6775673433928064 || Test AUC: 0.6258812579625708\n",
      "epoch: 12 || train_loss: 65318.07017081491 || valid_loss: 22274.145629383936 || Train AUC: 0.6786807144303282 || Test AUC: 0.626312661326593\n",
      "epoch: 13 || train_loss: 65246.09555027108 || valid_loss: 22270.813741492777 || Train AUC: 0.6796886273688783 || Test AUC: 0.6266370483038013\n",
      "epoch: 14 || train_loss: 65178.821088287536 || valid_loss: 22269.580966730413 || Train AUC: 0.6806237536737589 || Test AUC: 0.6268687051624602\n",
      "epoch: 15 || train_loss: 65114.181763547924 || valid_loss: 22269.97686836622 || Train AUC: 0.6815278142949437 || Test AUC: 0.6270243278614142\n",
      "epoch: 16 || train_loss: 65050.470655312616 || valid_loss: 22271.615049345044 || Train AUC: 0.6824228165083829 || Test AUC: 0.627079795432326\n",
      "Early stop at epoch: 16\n",
      "[0.44707798 0.44707798 0.44707798 ... 0.55194165 0.57992066 0.54436138]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    fm = FactorizationMachine(N = num_x,\n",
    "                              K=4,\n",
    "                              data = trainset,\n",
    "                              y = y,\n",
    "                              test_data = testset,\n",
    "                              lr=0.001,\n",
    "                              l2_reg=True,\n",
    "                              l2_lambda=0.0001,\n",
    "                              epoch=200,\n",
    "                              early_stop_window=3)\n",
    "\n",
    "    fm.train()\n",
    "    predict_proba = fm.inference(data=testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(len(predict_proba)):\n",
    "    if predict_proba[i] >= 0.5: #상승\n",
    "        result.append(1)\n",
    "    else:\n",
    "        result.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./processed/predict_results2013_2016.pickle', 'wb') as f:\n",
    "    pickle.dump(result, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seongju",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
